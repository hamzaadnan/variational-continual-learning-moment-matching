{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRy_bEI7sa1s"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HIif31eNKF_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.distributions import Normal, kl_divergence\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
        "from tqdm.auto import tqdm\n",
        "import copy\n",
        "import pickle\n",
        "import gzip\n",
        "import gdown\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX3yOCbNNqRL"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EqJBcRreBO2"
      },
      "outputs": [],
      "source": [
        "# drive.mount('/content/drive')\n",
        "# logdir = '/content/drive/MyDrive/Uncertainty in Deep Learning/Mini Project/Experiments'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models and Experiments"
      ],
      "metadata": {
        "id": "rvxzBXTuBWSt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdKfB4ifdScf"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDuSv92Z2WAF"
      },
      "outputs": [],
      "source": [
        "class PermutedMNIST:\n",
        "  def __init__(self, num_tasks=10, batch_size=256):\n",
        "    super().__init__()\n",
        "    path = 'mnist.pkl.gz'\n",
        "    if not os.path.exists(path):\n",
        "      file_id = '159W5BVgLAjv2T07RaQ2iMTJDQZoqQ_Mx'\n",
        "      url = f'https://drive.google.com/uc?id={file_id}'\n",
        "      data = gdown.download(url, quiet=True)\n",
        "\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "      train, valid, test = pickle.load(f, encoding='latin1')\n",
        "\n",
        "\n",
        "    train_data = torch.from_numpy(np.vstack((train[0], valid[0])))\n",
        "    train_targets = torch.from_numpy(np.hstack((train[1], valid[1])))\n",
        "\n",
        "    test_data, test_targets = torch.from_numpy(test[0]), torch.from_numpy(test[1])\n",
        "\n",
        "\n",
        "    self.train_data = TensorDataset(train_data.to(torch.float32), train_targets.to(torch.long))\n",
        "    self.test_data = TensorDataset(test_data.to(torch.float32), test_targets.to(torch.long))\n",
        "\n",
        "    self.num_tasks = num_tasks\n",
        "    self.batch_size = batch_size\n",
        "    self.current_task = 0\n",
        "\n",
        "  def next_task(self):\n",
        "    assert self.current_task < self.num_tasks, \"Maximum number of tasks completed.\"\n",
        "    if self.current_task > 0:\n",
        "      torch.manual_seed(self.current_task)\n",
        "      permutation = torch.randperm(784)\n",
        "      train_data = TensorDataset(self.train_data.tensors[0][:, permutation], self.train_data.tensors[1])\n",
        "      test_data = TensorDataset(self.test_data.tensors[0][:, permutation], self.test_data.tensors[1])\n",
        "    else:\n",
        "      train_data, test_data = self.train_data, self.test_data\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    self.current_task += 1\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ww3JgQGVT_eg"
      },
      "outputs": [],
      "source": [
        "class SplitMNIST:\n",
        "  def __init__(self, num_tasks=5, tasks=((0, 1), (2, 3), (4, 5), (6, 7), (8, 9)), batch_size=256):\n",
        "    super().__init__()\n",
        "    path = 'mnist.pkl.gz'\n",
        "    if not os.path.exists(path):\n",
        "      file_id = '159W5BVgLAjv2T07RaQ2iMTJDQZoqQ_Mx'\n",
        "      url = f'https://drive.google.com/uc?id={file_id}'\n",
        "      data = gdown.download(url, quiet=True)\n",
        "\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "      train, valid, test = pickle.load(f, encoding='latin1')\n",
        "\n",
        "    self.train_data = torch.from_numpy(np.vstack((train[0], valid[0])))\n",
        "    self.train_targets = torch.from_numpy(np.hstack((train[1], valid[1])))\n",
        "\n",
        "    self.test_data, self.test_targets = torch.from_numpy(test[0]), torch.from_numpy(test[1])\n",
        "\n",
        "    self.num_tasks = num_tasks\n",
        "    self.tasks = tasks\n",
        "    self.batch_size = batch_size\n",
        "    self.current_task = 0\n",
        "\n",
        "  def next_task(self):\n",
        "    assert self.current_task < self.num_tasks, \"Maximum number of tasks completed\"\n",
        "    train_indices_0 = torch.where(\n",
        "        self.train_targets == self.tasks[self.current_task][0],\n",
        "    )[0]\n",
        "    train_indices_1 = torch.where(\n",
        "        self.train_targets == self.tasks[self.current_task][1],\n",
        "    )[0]\n",
        "    test_indices_0 = torch.where(\n",
        "        self.test_targets == self.tasks[self.current_task][0],\n",
        "    )[0]\n",
        "    test_indices_1 = torch.where(\n",
        "        self.test_targets == self.tasks[self.current_task][1],\n",
        "    )[0]\n",
        "\n",
        "    train_targets_0 = torch.zeros(train_indices_0.size(0), dtype=torch.float32)\n",
        "    train_targets_1 = torch.ones(train_indices_1.size(0), dtype=torch.float32)\n",
        "\n",
        "    test_targets_0 = torch.zeros(test_indices_0.size(0), dtype=torch.float32)\n",
        "    test_targets_1 = torch.ones(test_indices_1.size(0), dtype=torch.float32)\n",
        "\n",
        "    train_data = torch.cat((self.train_data[train_indices_0], self.train_data[train_indices_1]), dim=0).to(torch.float32)\n",
        "    test_data = torch.cat((self.test_data[test_indices_0], self.test_data[test_indices_1]), dim=0).to(torch.float32)\n",
        "\n",
        "    train_targets = torch.cat((train_targets_0, train_targets_1), dim=0)\n",
        "    test_targets = torch.cat((test_targets_0, test_targets_1), dim=0)\n",
        "\n",
        "    train_data = TensorDataset(train_data, train_targets)\n",
        "    test_data = TensorDataset(test_data, test_targets)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    self.current_task += 1\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVEZW8TJ6Ay8"
      },
      "outputs": [],
      "source": [
        "def random_coreset_permuted(loader, coreset_size):\n",
        "  batch_size = loader.batch_size\n",
        "  batch_coreset_size = coreset_size // len(loader)\n",
        "  final_coreset_size = coreset_size % len(loader)\n",
        "\n",
        "  num_samples = loader.dataset.tensors[0].size(0)\n",
        "  data_size = loader.dataset.tensors[0].size(1)\n",
        "  target_dtype = loader.dataset.tensors[1].dtype\n",
        "\n",
        "  data_coreset = torch.zeros(size=(coreset_size, data_size), dtype=torch.float32)\n",
        "  targets_coreset = torch.zeros(size=(coreset_size,), dtype=target_dtype)\n",
        "\n",
        "  data_ = []\n",
        "  targets_ = []\n",
        "  coreset_index = 0\n",
        "  for idx, (data, target) in enumerate(loader):\n",
        "    sz = min(batch_coreset_size if idx < len(loader) - 1 else final_coreset_size, data.size(0))\n",
        "    permutation = torch.randperm(data.size(0))\n",
        "    selection_indices = permutation[:sz]\n",
        "    remaining_indices = permutation[sz:]\n",
        "\n",
        "    cs_data = data[selection_indices]\n",
        "    cs_target = target[selection_indices]\n",
        "    data_.append(data[remaining_indices])\n",
        "    targets_.append(target[remaining_indices])\n",
        "\n",
        "    data_coreset[coreset_index:coreset_index + sz] = cs_data\n",
        "    targets_coreset[coreset_index:coreset_index + sz] = cs_target\n",
        "    coreset_index += sz\n",
        "\n",
        "  coreset_dataset = TensorDataset(data_coreset, targets_coreset)\n",
        "  coreset_loader = DataLoader(coreset_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  data_, targets_ = torch.cat(data_, dim=0), torch.cat(targets_, dim=0)\n",
        "  augmented_dataset = TensorDataset(data_, targets_.to(target_dtype))\n",
        "  augmented_loader = DataLoader(augmented_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return coreset_loader, augmented_loader\n",
        "\n",
        "\n",
        "\n",
        "def merge_loaders(loaders):\n",
        "  coreset_data = []\n",
        "  coreset_targets = []\n",
        "  batch_size = loaders[0].batch_size\n",
        "  data_dtype = loaders[0].dataset.tensors[0].dtype\n",
        "  target_dtype = loaders[0].dataset.tensors[1].dtype\n",
        "\n",
        "  for loader in loaders:\n",
        "    data, targets = loader.dataset.tensors\n",
        "    data, targets = data.detach(), targets.detach()\n",
        "    coreset_data.append(data)\n",
        "    coreset_targets.append(targets)\n",
        "\n",
        "  coreset_data = torch.cat(coreset_data, dim=0)\n",
        "  coreset_targets = torch.cat(coreset_targets, dim=0)\n",
        "\n",
        "  merged_dataset = TensorDataset(coreset_data.to(data_dtype), coreset_targets.to(target_dtype))\n",
        "  merged_loader = DataLoader(merged_dataset, batch_size=batch_size, shuffle=True)\n",
        "  return merged_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F-O0-ROvEDC"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlgjtfn-xZ4w"
      },
      "outputs": [],
      "source": [
        "class VanillaNN(nn.Module):\n",
        "  def __init__(self, input_dim=784, hidden_dims=[100, 100], output_dim=10):\n",
        "    super(VanillaNN, self).__init__()\n",
        "    self.layers = nn.ModuleList()\n",
        "    prev_dim = input_dim\n",
        "    for hidden_dim in hidden_dims:\n",
        "      self.layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "      prev_dim = hidden_dim\n",
        "    self.output = nn.Linear(prev_dim, output_dim)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = self.relu(layer(x))\n",
        "    x = self.output(x)\n",
        "    return x\n",
        "\n",
        "  def get_weights(self):\n",
        "    weight_dict = {}\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      weight_dict[f'W_{i}'] = layer.weight.data.detach().clone()\n",
        "      weight_dict[f'b_{i}'] = layer.bias.data.detach().clone()\n",
        "    weight_dict[f'W_out'] = self.output.weight.data.detach().clone()\n",
        "    weight_dict[f'b_out'] = self.output.bias.data.detach().clone()\n",
        "    return weight_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mK-VzL3iDwm"
      },
      "outputs": [],
      "source": [
        "class BayesianLinear(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(BayesianLinear, self).__init__()\n",
        "\n",
        "    self.W_mu = nn.Parameter(torch.Tensor(size=(output_dim, input_dim)))\n",
        "    self.W_logvar = nn.Parameter(torch.Tensor(size=(output_dim, input_dim)))\n",
        "    self.b_mu = nn.Parameter(torch.Tensor(size=(output_dim,)))\n",
        "    self.b_logvar = nn.Parameter(torch.Tensor(size=(output_dim,)))\n",
        "    self._reset_parameters()\n",
        "\n",
        "    self.register_buffer('W_mu_prior', torch.zeros_like(self.W_mu))\n",
        "    self.register_buffer('W_logvar_prior', torch.zeros_like(self.W_logvar))\n",
        "    self.register_buffer('b_mu_prior', torch.zeros_like(self.b_mu))\n",
        "    self.register_buffer('b_logvar_prior', torch.zeros_like(self.b_logvar))\n",
        "\n",
        "  def forward(self, x):\n",
        "    W_var = torch.exp(self.W_logvar)\n",
        "    b_var = torch.exp(self.b_logvar)\n",
        "\n",
        "    mu_act = F.linear(x, self.W_mu, self.b_mu)\n",
        "    var_act = F.linear(x ** 2, W_var, b_var)\n",
        "    eps = torch.randn_like(mu_act)\n",
        "\n",
        "    return mu_act + torch.sqrt(var_act) * eps\n",
        "\n",
        "  def update_priors(self, prior_dict=None):\n",
        "    if prior_dict is not None:\n",
        "      self.W_mu_prior.data.copy_(prior_dict['W_mu_prior'])\n",
        "      self.W_logvar_prior.data.copy_(prior_dict['W_logvar_prior'])\n",
        "\n",
        "      self.b_mu_prior.data.copy_(prior_dict['b_mu_prior'])\n",
        "      self.b_logvar_prior.data.copy_(prior_dict['b_logvar_prior'])\n",
        "    else:\n",
        "      self.W_mu_prior.data.copy_(self.W_mu.data)\n",
        "      self.W_logvar_prior.data.copy_(self.W_logvar.data)\n",
        "\n",
        "      self.b_mu_prior.data.copy_(self.b_mu.data)\n",
        "      self.b_logvar_prior.data.copy_(self.b_logvar.data)\n",
        "\n",
        "  def _reset_parameters(self):\n",
        "    self.W_mu.data.normal_(0, 0.1)\n",
        "    self.W_logvar.data.fill_(-6)\n",
        "\n",
        "    self.b_mu.data.normal_(0, 0.1)\n",
        "    self.b_logvar.data.fill_(-6)\n",
        "\n",
        "  def kl_loss(self):\n",
        "    W_var = torch.exp(self.W_logvar)\n",
        "    W_var_prior = torch.exp(self.W_logvar_prior)\n",
        "\n",
        "    b_var = torch.exp(self.b_logvar)\n",
        "    b_var_prior = torch.exp(self.b_logvar_prior)\n",
        "\n",
        "    kl_W = 0.5 * (torch.log((W_var_prior / W_var)) + (W_var / W_var_prior) + \\\n",
        "                  ((self.W_mu - self.W_mu_prior) ** 2 / W_var_prior) - 1).sum()\n",
        "    kl_b = 0.5 * (torch.log((b_var_prior / b_var)) + (b_var / b_var_prior) + \\\n",
        "                  ((self.b_mu - self.b_mu_prior) ** 2 / b_var_prior) - 1).sum()\n",
        "\n",
        "    return kl_W + kl_b\n",
        "\n",
        "  def wasserstein_loss_1(self):\n",
        "    W_sigma = torch.exp(0.5 * self.W_logvar)\n",
        "    W_sigma_prior = torch.exp(0.5 * self.W_logvar_prior)\n",
        "\n",
        "    b_sigma = torch.exp(0.5 * self.b_logvar)\n",
        "    b_sigma_prior = torch.exp(0.5 * self.b_logvar_prior)\n",
        "\n",
        "    wasserstein_W = torch.abs(self.W_mu - self.W_mu_prior).sum() + torch.abs(W_sigma - W_sigma_prior).sum()\n",
        "    wasserstein_b = torch.abs(self.b_mu - self.b_mu_prior).sum() + torch.abs(b_sigma - b_sigma_prior).sum()\n",
        "\n",
        "    return wasserstein_W + wasserstein_b\n",
        "\n",
        "  def wasserstein_loss_2(self):\n",
        "    W_sigma = torch.exp(0.5 * self.W_logvar)\n",
        "    W_sigma_prior = torch.exp(0.5 * self.W_logvar_prior)\n",
        "\n",
        "    b_sigma = torch.exp(0.5 * self.b_logvar)\n",
        "    b_sigma_prior = torch.exp(0.5 * self.b_logvar_prior)\n",
        "\n",
        "    wasserstein_W = (self.W_mu - self.W_mu_prior).pow(2).sum() + (W_sigma - W_sigma_prior).pow(2).sum()\n",
        "    wasserstein_b = (self.b_mu - self.b_mu_prior).pow(2).sum() + (b_sigma - b_sigma_prior).pow(2).sum()\n",
        "\n",
        "    return wasserstein_W + wasserstein_b\n",
        "\n",
        "  def initialise_weights(self, W_mu, b_mu):\n",
        "    self.W_mu_prior.data.copy_(W_mu)\n",
        "    self.b_mu_prior.data.copy_(b_mu)\n",
        "\n",
        "    # self.W_mu.data = W_mu.detach().clone()\n",
        "    # self.b_mu.data = b_mu.detach().clone()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyKvKn-YqBaf"
      },
      "outputs": [],
      "source": [
        "class BayesianNet(nn.Module):\n",
        "  def __init__(self, input_dim=784, hidden_dims=[100, 100], output_dim=10, single_head=True, num_tasks=10):\n",
        "    super(BayesianNet, self).__init__()\n",
        "    self.single_head = single_head\n",
        "    self.num_tasks = num_tasks\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.output_dim = output_dim\n",
        "    prev_dim = input_dim\n",
        "    for hidden_dim in hidden_dims:\n",
        "      self.layers.append(BayesianLinear(prev_dim, hidden_dim))\n",
        "      prev_dim = hidden_dim\n",
        "    self.output = BayesianLinear(prev_dim, output_dim) if single_head else \\\n",
        "      nn.ModuleList([BayesianLinear(prev_dim, output_dim) for _ in range(num_tasks)])\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x, task_num=None):\n",
        "    for layer in self.layers:\n",
        "      x = self.relu(layer(x))\n",
        "    x = self.output(x) if self.single_head else self.output[task_num](x)\n",
        "    return x\n",
        "\n",
        "  def kl_loss(self):\n",
        "    kl_div = 0.\n",
        "    for layer in self.layers:\n",
        "      kl_div += layer.kl_loss()\n",
        "    kl_div += self.output.kl_loss() if self.single_head else sum([output.kl_loss() for output in self.output])\n",
        "    return kl_div\n",
        "\n",
        "  def wasserstein_loss_1(self):\n",
        "    wasserstein_div = 0.\n",
        "    for layer in self.layers:\n",
        "      wasserstein_div += layer.wasserstein_loss_1()\n",
        "    wasserstein_div += self.output.wasserstein_loss_1() if self.single_head else sum([output.wasserstein_loss_1() for output in self.output])\n",
        "    return wasserstein_div\n",
        "\n",
        "  def wasserstein_loss_2(self):\n",
        "    wasserstein_div = 0.\n",
        "    for layer in self.layers:\n",
        "      wasserstein_div += layer.wasserstein_loss_2()\n",
        "    wasserstein_div += self.output.wasserstein_loss_2() if self.single_head else sum([output.wasserstein_loss_2() for output in self.output])\n",
        "    return wasserstein_div\n",
        "\n",
        "  def update_priors(self, task):\n",
        "    for layer in self.layers:\n",
        "      layer.update_priors()\n",
        "    if self.single_head:\n",
        "      self.output.update_priors()\n",
        "    else:\n",
        "      self.output[task].update_priors()\n",
        "      # if self.num_tasks > 1 and task < self.num_tasks - 1:\n",
        "      #   prior_dict = {\n",
        "      #       'W_mu_prior': self.output[task].W_mu.data.detach().clone(),\n",
        "      #       'W_logvar_prior': self.output[task].W_logvar.data.detach().clone(),\n",
        "      #       'b_mu_prior': self.output[task].b_mu.data.detach().clone(),\n",
        "      #       'b_logvar_prior': self.output[task].b_logvar.data.detach().clone()\n",
        "      #     }\n",
        "      #   self.output[task + 1].update_priors(prior_dict)\n",
        "\n",
        "  def initialise_layers(self, weight_dict):\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      layer.initialise_weights(weight_dict[f'W_{i}'], weight_dict[f'b_{i}'])\n",
        "    if self.single_head:\n",
        "      self.output.initialise_weights(weight_dict[f'W_out'], weight_dict[f'b_out'])\n",
        "    else:\n",
        "      self.output[0].initialise_weights(weight_dict[f'W_out'], weight_dict[f'b_out'])\n",
        "      # if self.num_tasks > 1:\n",
        "      #   self.output[1].initialise_weights(weight_dict[f'W_out'], weight_dict[f'b_out'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Tp-LKJjBpl"
      },
      "source": [
        "### Classification Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKNouOJcCXkz"
      },
      "outputs": [],
      "source": [
        "def bayesian_loss(model, data, target, task_num, single_head, loss_mode, reg='kl', lambd=0.1, num_samples=30):\n",
        "  nll_loss = 0.\n",
        "  if loss_mode == 'classification':\n",
        "    nll_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "  elif loss_mode == 'binary_classification':\n",
        "    nll_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "  else:\n",
        "    nll_fn = nn.MSELoss(reduction='mean')\n",
        "  for _ in range(num_samples):\n",
        "    pred = model(data) if single_head else model(data, task_num)\n",
        "    nll_loss += nll_fn(pred, target)\n",
        "\n",
        "  nll_loss /= num_samples\n",
        "  if reg == 'ws_1':\n",
        "    reg_loss = model.wasserstein_loss_1()\n",
        "  elif reg == 'ws_2':\n",
        "    reg_loss = model.wasserstein_loss_2()\n",
        "  else:\n",
        "    reg_loss = model.kl_loss()\n",
        "  reg_loss /= data.size(0)\n",
        "  return nll_loss + lambd * reg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylXDkRQO9Ajj"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(model, data, target, task_num, single_head, loss_mode='classification', num_samples=10):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    if single_head:\n",
        "      output = torch.stack([model(data) for _ in range(num_samples)], dim=0)\n",
        "    else:\n",
        "      output = torch.stack([model(data, task_num) for _ in range(num_samples)], dim=0)\n",
        "    output = torch.mean(output, dim=0)\n",
        "    if loss_mode == 'binary_classification':\n",
        "      predicted = (torch.sigmoid(output) > 0.5).float()\n",
        "      predicted = predicted.view(-1)\n",
        "      target = target.view(-1)\n",
        "    else:\n",
        "      predicted = torch.argmax(output, dim=-1)\n",
        "    return (predicted == target).float().mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHecqKWjjFsc"
      },
      "source": [
        "### Training/Testing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFYSbn3M_ZoV"
      },
      "outputs": [],
      "source": [
        "def train_vanilla(model, optimiser, train_loader, loss_mode='classification', num_epochs=50):\n",
        "  if loss_mode == 'classification':\n",
        "    nll_fn = nn.CrossEntropyLoss(reduction='mean')\n",
        "  elif loss_mode == 'binary_classification':\n",
        "    nll_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "  else:\n",
        "    nll_fn = nn.MSELoss(reduction='mean')\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for _, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "      optimiser.zero_grad()\n",
        "      pred = model(data)\n",
        "      if loss_mode == 'binary_classification':\n",
        "        target = target.unsqueeze(dim=-1)\n",
        "      if loss_mode == 'regression':\n",
        "        target = F.one_hot(target.to(torch.long), num_classes=model.output_dim).to(torch.float32)\n",
        "      loss = nll_fn(pred, target)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "\n",
        "def eval_vanilla(model, test_loader):\n",
        "  model.eval()\n",
        "  test_accuracy = 0.\n",
        "  with torch.no_grad():\n",
        "    for _, (data, target) in enumerate(test_loader):\n",
        "      data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "      test_accuracy += model(data).argmax(dim=-1).eq(target).float().mean().item()\n",
        "  return test_accuracy / len(test_loader)\n",
        "\n",
        "\n",
        "def train_bayesian(model, optimiser, train_loader, task_num, single_head, loss_mode='classification', reg='kl', num_epochs=50, lambd=0.5, num_samples=30):\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for idx, (data, target) in enumerate(train_loader):\n",
        "      data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "      if loss_mode == 'binary_classification':\n",
        "        target = target.unsqueeze(dim=-1)\n",
        "      if loss_mode == 'regression':\n",
        "        target = F.one_hot(target.to(torch.long), num_classes=model.output_dim).to(torch.float32)\n",
        "      optimiser.zero_grad()\n",
        "      loss = bayesian_loss(model, data, target, task_num, single_head, loss_mode=loss_mode, reg=reg, lambd=lambd, num_samples=num_samples)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "\n",
        "def eval_bayesian(model, test_loader, task_num, single_head, loss_mode, num_samples=30):\n",
        "  model.eval()\n",
        "  test_accuracy = 0.\n",
        "  with torch.no_grad():\n",
        "    for idx, (data, target) in enumerate(test_loader):\n",
        "      data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "      test_accuracy += compute_accuracy(model, data, target, task_num, single_head, loss_mode=loss_mode, num_samples=num_samples)\n",
        "  return test_accuracy / len(test_loader)\n",
        "\n",
        "def regression_loss(model, test_loader, task_num, single_head, loss_mode, num_samples=30):\n",
        "  model.eval()\n",
        "  test_loss = 0.\n",
        "  with torch.no_grad():\n",
        "    for idx, (data, target) in enumerate(test_loader):\n",
        "      data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "      if single_head:\n",
        "        output = torch.stack([model(data) for _ in range(num_samples)], dim=0)\n",
        "      else:\n",
        "        output = torch.stack([model(data, task_num) for _ in range(num_samples)], dim=0)\n",
        "      output = torch.mean(output, dim=0)\n",
        "      num_classes = output.size(1)\n",
        "      target = F.one_hot(target.to(torch.long), num_classes=num_classes).to(torch.float32)\n",
        "      test_loss += torch.sqrt(F.mse_loss(output, target)).item()\n",
        "  return test_loss / len(test_loader)\n",
        "\n",
        "\n",
        "# def eval_tasks(model, test_loader, single_head, num_samples=5, is_bayesian=True):\n",
        "#   # test_accuracies = []\n",
        "#   # for i, test_loader in enumerate(test_loaders):\n",
        "#   if is_bayesian:\n",
        "#    return eval_bayesian(model, test_loader, task_num=i, single_head=single_head, num_samples=num_samples)\n",
        "#   else:\n",
        "#     return eval_vanilla(model, test_loader)\n",
        "#   # return np.mean(test_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy4bksVOlBIq"
      },
      "outputs": [],
      "source": [
        "def vcl(generator=PermutedMNIST, num_tasks=10, single_head=True, num_epochs=30, lambd=[1e-3], num_samples=5, \\\n",
        "                        lr=1e-3, loss_mode='classification', reg='kl', batch_size=256, \\\n",
        "                        update_priors=True, input_dim=784, hidden_dims=[100, 100], output_dim=10, \\\n",
        "                        coreset_size=0, use_vanilla=False, verbose=True, plot_output=True):\n",
        "\n",
        "  accuracy_matrix = np.zeros((2*num_tasks, num_tasks)) if loss_mode == 'regression' else np.zeros((num_tasks, num_tasks))\n",
        "  generator = generator(num_tasks=num_tasks, batch_size=batch_size)\n",
        "  train_loaders, test_loaders = [], []\n",
        "  coreset_loaders = []\n",
        "  accuracies = []\n",
        "\n",
        "  model_bayesian = BayesianNet(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim, \\\n",
        "                      single_head=single_head, num_tasks=num_tasks).to(DEVICE)\n",
        "  optimiser_bayesian = torch.optim.Adam(model_bayesian.parameters(), lr=lr)\n",
        "\n",
        "  if use_vanilla:\n",
        "    model_vanilla = VanillaNN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim).to(DEVICE)\n",
        "    optimiser_vanilla = torch.optim.Adam(model_vanilla.parameters(), lr=lr)\n",
        "    train_loader, test_loader = generator.next_task()\n",
        "    train_loaders.append(train_loader)\n",
        "    test_loaders.append(test_loader)\n",
        "    train_vanilla(model_vanilla, optimiser_vanilla, train_loader, loss_mode=loss_mode, num_epochs=num_epochs)\n",
        "    model_bayesian.initialise_layers(model_vanilla.get_weights())\n",
        "\n",
        "  for task in tqdm(range(num_tasks)):\n",
        "    if task == 0 and use_vanilla:\n",
        "      train_loader, test_loader = train_loaders[0], test_loaders[0]\n",
        "    else:\n",
        "      train_loader, test_loader = generator.next_task()\n",
        "    if coreset_size > 0:\n",
        "      coreset_loader, train_loader = random_coreset_permuted(train_loader, coreset_size)\n",
        "      coreset_loaders.append(coreset_loader)\n",
        "      if task > 0 and single_head:\n",
        "        train_loader = merge_loaders([train_loader, coreset_loaders[task - 1]])\n",
        "\n",
        "    train_loaders.append(train_loader)\n",
        "    test_loaders.append(test_loader)\n",
        "\n",
        "    print(f'Task {task + 1} / {num_tasks}')\n",
        "    train_loader, test_loader = train_loaders[task], test_loaders[task]\n",
        "    train_bayesian(model_bayesian, optimiser_bayesian, train_loader, task_num=task, single_head=single_head, \\\n",
        "                     loss_mode=loss_mode, reg=reg, num_epochs=num_epochs, lambd=lambd[task], num_samples=num_samples)\n",
        "    if update_priors:\n",
        "      model_bayesian.update_priors(task)\n",
        "    # inference_model = BayesianNet(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim, \\\n",
        "    #                   single_head=single_head, num_tasks=num_tasks).to(DEVICE)\n",
        "    # inference_model.load_state_dict(model_bayesian.state_dict())\n",
        "    # inference_optimiser = torch.optim.Adam(inference_model.parameters(), lr=lr)\n",
        "\n",
        "    if coreset_size > 0:\n",
        "      if single_head:\n",
        "        merged_coreset = merge_loaders(coreset_loaders) if task > 0 else coreset_loaders[0]\n",
        "        train_bayesian(model_bayesian, optimiser_bayesian, merged_coreset, task_num=task, single_head=single_head, \\\n",
        "                        loss_mode=loss_mode, reg=reg, num_epochs=num_epochs, lambd=lambd[task], num_samples=num_samples)\n",
        "      else:\n",
        "        train_bayesian(model_bayesian, optimiser_bayesian, coreset_loaders[task], task_num=task, single_head=single_head, \\\n",
        "                        loss_mode=loss_mode, reg=reg, num_epochs=num_epochs, lambd=lambd[task], num_samples=num_samples)\n",
        "\n",
        "    for i in range(task + 1):\n",
        "      accuracy_matrix[task, i] = eval_bayesian(model_bayesian, test_loaders[i], i, single_head=single_head, loss_mode=loss_mode, num_samples=num_samples)\n",
        "      if loss_mode == 'regression':\n",
        "        accuracy_matrix[num_tasks + task, i] = regression_loss(model_bayesian, test_loaders[i], i, single_head=single_head, loss_mode=loss_mode, num_samples=num_samples)\n",
        "    accuracies.append(np.mean(accuracy_matrix[task, :task+1]))\n",
        "\n",
        "    if verbose:\n",
        "      print(f'Accuracies at task {task + 1} = {accuracy_matrix[task, :]}')\n",
        "      print(f'Average accuracy at task {task + 1} = {accuracies[-1]}')\n",
        "      print('\\n')\n",
        "\n",
        "\n",
        "  if plot_output:\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    _ = ax.plot(range(1, num_tasks + 1), accuracies, marker='o', label=f'Coreset Size = {coreset_size}')\n",
        "    _ = ax.set_xlabel('Task')\n",
        "    _ = ax.set_ylabel('Accuracy')\n",
        "    _ = ax.set_title('Accuracy vs Number of Tasks')\n",
        "    _ = ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "  return accuracy_matrix, accuracies, model_bayesian"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYvS1opxRmVQ"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqidJ-doHauo"
      },
      "source": [
        "#### Permuted MNIST Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGCuEnNUf5mS"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'classification'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "KL_LAMBDA = [0. for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "USE_VANILLA = False\n",
        "LR = 1e-4\n",
        "UPDATE_PRIORS = False\n",
        "\n",
        "accuracy_matrix_baseline, avg_accuracies_baseline, model_baseline = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           batch_size=BATCH_SIZE, update_priors=UPDATE_PRIORS, num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, \\\n",
        "                                           num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, plot_output=True)\n",
        "\n",
        "np.save(os.path.join(logdir, 'accuracy_matrix_permuted_baseline'), accuracy_matrix_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIU_JZfhjT0Q"
      },
      "source": [
        "#### Permuted MNIST (Without Coresets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKvJ4MCroWG_"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'classification'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "KL_LAMBDA = [2e-2 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-4\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lvppRoCgNmA"
      },
      "source": [
        "#### Permuted MNIST (With Coresets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sXtI59Ziib7"
      },
      "source": [
        "##### Coreset Size = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtOLEtGLh2k8"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'classification'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "KL_LAMBDA = [2e-2 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 200\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-4\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True)\n",
        "\n",
        "# np.save(os.path.join(logdir, f'accuracy_matrix_permuted_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxMzWucKigdG"
      },
      "source": [
        "##### Coreset Size = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjDf7L5aiLr2"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'classification'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "KL_LAMBDA = [2e-2 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 400\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-4\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCahi09vieeR"
      },
      "source": [
        "##### Coreset Size = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swiaEZu9ib-6"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'classification'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "KL_LAMBDA = [2e-2 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 1000\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-4\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=True, plot_output=True)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC5Pntn5R8Ro"
      },
      "source": [
        "#### Split MNIST (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxmkfdTuR9ze"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'binary_classification'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [0 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "UPDATE_PRIORS = False\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=1)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_baseline'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BozR76Xwi3ZU"
      },
      "source": [
        "#### Split MNIST (VCL - No Coreset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xx8r-BSDi7OY"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'binary_classification'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [1e-1 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=1)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE5-ki-rjAuL"
      },
      "source": [
        "#### SplitMNIST (VCL - Coresets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jyQrd-vjFcy"
      },
      "source": [
        "##### Coreset Size = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFUHvgrDjDv_"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'binary_classification'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [1e-1 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 200\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=1)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqDed2hRjIVs"
      },
      "source": [
        "##### Coreset Size = 400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7S6n3zpjKQ3"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'binary_classification'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [1e-1 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 400\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=1)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7YLBX1HjKuB"
      },
      "source": [
        "##### Coreset Size 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpYzj3A4jMZV"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'binary_classification'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [1e-1 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 1000\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=1)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrP3P-P-N17K"
      },
      "source": [
        "#### Regression Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yLD6uqWN-5m"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyM8QkeoN_1w"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 100\n",
        "BATCH_SIZE = 256\n",
        "LAMBD = [0. for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "USE_VANILLA = False\n",
        "LR = 1e-4\n",
        "UPDATE_PRIORS = False\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "accuracy_matrix_baseline, avg_accuracies_baseline, model_baseline = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           batch_size=BATCH_SIZE, update_priors=UPDATE_PRIORS, num_epochs=NUM_EPOCHS, lambd=LAMBD, \\\n",
        "                                           num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, plot_output=True, output_dim=OUTPUT_DIM)\n",
        "\n",
        "np.save(os.path.join(logdir, 'accuracy_matrix_permuted_regression_baseline'), accuracy_matrix_baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5pq1pMePPE2"
      },
      "source": [
        "##### Permuted MNIST (Without Coresets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7wl8vvyPRvr"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 256\n",
        "KL_LAMBDA = [0.0] + [1e-4 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True, output_dim=OUTPUT_DIM)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_regression_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTYxk19oPZoX"
      },
      "source": [
        "##### Permuted MNIST (With Coresets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 256\n",
        "KL_LAMBDA = [0.0] + [1e-4 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 200\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True, output_dim=OUTPUT_DIM)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_regression_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ],
      "metadata": {
        "id": "Og_pmV0S1G53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1je_O7Q7Pcdt"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 256\n",
        "KL_LAMBDA = [0.0] + [1e-4 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 400\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True, output_dim=OUTPUT_DIM)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_regression_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhQYcHboPc1c"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 256\n",
        "KL_LAMBDA = [0.0] + [1e-4 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 1000\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True, output_dim=OUTPUT_DIM)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_regression_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLwRH18NPiWj"
      },
      "source": [
        "##### Split MNIST (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbE_JPbXPnoo"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [0 for _ in range(NUM_TASKS)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "UPDATE_PRIORS = False\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=2)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_regression_baseline'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37VC6RghQW8p"
      },
      "source": [
        "##### Split MNIST (VCL - No Coresets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdBBdGFSQbNk"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=2)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_regression_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu6tI0yEQyQm"
      },
      "source": [
        "##### Split MNIST (VCL - Coresets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGiQV5JYQ0xe"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 200\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=2)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_regression_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo_JEuG3Q2vi"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 400\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=2)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_regression_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRHMiYY0Q4AP"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'regression'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 25\n",
        "KL_LAMBDA = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 1000\n",
        "UPDATE_PRIORS = True\n",
        "LR = 3e-4\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=False, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=2)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_regression_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI6nm5E-RNsD"
      },
      "source": [
        "#### Wasserstein Loss Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCLAW6Piakpy"
      },
      "outputs": [],
      "source": [
        "NLL_TYPE = 'classification'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 256\n",
        "LAMBD = [0.0] + [5.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "REG = 'ws_2'\n",
        "USE_VANILLA = True\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-3\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=LAMBD, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, reg=REG, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_reg_{REG}_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NLL_TYPE = 'classification'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 256\n",
        "LAMBD = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 200\n",
        "REG = 'ws_2'\n",
        "USE_VANILLA = True\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-3\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=LAMBD, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, reg=REG, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_reg_{REG}_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ],
      "metadata": {
        "id": "fMvJi4LZh2Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLL_TYPE = 'classification'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 256\n",
        "LAMBD = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 400\n",
        "REG = 'ws_2'\n",
        "USE_VANILLA = True\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-3\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=LAMBD, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, reg=REG, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_reg_{REG}_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ],
      "metadata": {
        "id": "-0nYWNsTrP_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLL_TYPE = 'classification'\n",
        "GENERATOR = PermutedMNIST\n",
        "NUM_TASKS = 10\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 256\n",
        "LAMBD = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = True\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 1000\n",
        "REG = 'ws_2'\n",
        "USE_VANILLA = True\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-3\n",
        "\n",
        "accuracy_matrix_permuted, avg_accuracies_permuted, model_permuted = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=LAMBD, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, reg=REG, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, update_priors=UPDATE_PRIORS, plot_output=True)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_permuted_reg_{REG}_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_permuted)"
      ],
      "metadata": {
        "id": "rMwkIo3PrRyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Split MNIST"
      ],
      "metadata": {
        "id": "-uRTSsdFh9wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NLL_TYPE = 'binary_classification'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 5\n",
        "KL_LAMBDA = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 0\n",
        "REG = 'ws_2'\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-3\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, reg=REG, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=1)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_reg_{REG}_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ],
      "metadata": {
        "id": "2Lh8KBPgiAHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLL_TYPE = 'binary_classification'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 5\n",
        "KL_LAMBDA = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 200\n",
        "REG = 'ws_2'\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-3\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, reg=REG, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=1)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_reg_{REG}_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ],
      "metadata": {
        "id": "IRuRSy2QiWgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLL_TYPE = 'binary_classification'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 5\n",
        "KL_LAMBDA = [0.0] + [1.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 400\n",
        "REG = 'ws_2'\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-3\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, reg=REG, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=1)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_reg_{REG}_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ],
      "metadata": {
        "id": "jQ2EAMHniXdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NLL_TYPE = 'binary_classification'\n",
        "GENERATOR = SplitMNIST\n",
        "NUM_TASKS = 5\n",
        "NUM_EPOCHS = 5\n",
        "KL_LAMBDA = [0.0] + [2.0 for _ in range(NUM_TASKS - 1)]\n",
        "SINGLE_HEAD = False\n",
        "NUM_SAMPLES = 10\n",
        "CORESET_SIZE = 1000\n",
        "REG = 'ws_2'\n",
        "USE_VANILLA = False\n",
        "UPDATE_PRIORS = True\n",
        "LR = 1e-3\n",
        "\n",
        "accuracy_matrix_split, avg_accuracies_split, model_split = vcl(generator=GENERATOR, num_tasks=NUM_TASKS, single_head=SINGLE_HEAD, \\\n",
        "                                           num_epochs=NUM_EPOCHS, lambd=KL_LAMBDA, num_samples=NUM_SAMPLES, lr=LR, loss_mode=NLL_TYPE, reg=REG, \\\n",
        "                                           coreset_size=CORESET_SIZE, use_vanilla=USE_VANILLA, plot_output=True, update_priors=UPDATE_PRIORS, output_dim=1)\n",
        "\n",
        "np.save(os.path.join(logdir, f'accuracy_matrix_split_reg_{REG}_vcl_coreset_{CORESET_SIZE}'), accuracy_matrix_split)"
      ],
      "metadata": {
        "id": "Md4zXfzEikQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "E0lzJGd-VMmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'Experiments'\n",
        "if not os.path.exists(path):\n",
        "  file_id = '1BgOfdQQYw0_TTjHZeZ8VOkkDtzr_LZES'\n",
        "  url = f'https://drive.google.com/drive/folders/1BgOfdQQYw0_TTjHZeZ8VOkkDtzr_LZES?usp=sharing'\n",
        "  data = gdown.download_folder(url, quiet=True)\n",
        "\n",
        "\n",
        "accuracy_matrices = {}\n",
        "for file in os.listdir(path):\n",
        "  if file.endswith('.npy'):\n",
        "    accuracy_matrices[file] = np.load(os.path.join(path, file))"
      ],
      "metadata": {
        "id": "VQidYy4jVOFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cl_metrics(task_matrix):\n",
        "  num_tasks = task_matrix.shape[0]\n",
        "  avg_accuracy = np.mean(task_matrix[-1, :])\n",
        "  BWT = np.mean([task_matrix[-1, i] - task_matrix[i, i] for i in range(num_tasks - 1)])\n",
        "  FM = np.mean([np.max(task_matrix[i+1:, i]) - task_matrix[-1, i] for i in range(num_tasks - 1)])\n",
        "  task_accuracies = [np.mean(task_matrix[i, :i+1]) for i in range(num_tasks)]\n",
        "\n",
        "  return task_accuracies, avg_accuracy, BWT, FM\n",
        "\n",
        "\n",
        "def plot_accuracies(accuracies, experiment_name, is_regression=False):\n",
        "  accuracy_baseline = accuracies[0]\n",
        "  accuracy_vcl_0 = accuracies[1]\n",
        "  accuracy_vcl_200 = accuracies[2]\n",
        "  accuracy_vcl_400 = accuracies[3]\n",
        "  accuracy_vcl_1000 = accuracies[4]\n",
        "\n",
        "  T = len(accuracy_baseline)\n",
        "\n",
        "  textwidth_in_pt = 455.24411\n",
        "  pt_to_inch = 1.0 / 72.27\n",
        "\n",
        "  width_in = 0.6 * textwidth_in_pt * pt_to_inch\n",
        "  line_height_pt = 12\n",
        "  height_in = 3.5 * line_height_pt * pt_to_inch\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(8, 4))\n",
        "  ax.set_title(experiment_name)\n",
        "  ax.set_xlabel('Task')\n",
        "  ax.set_ylabel('Accuracy') if not is_regression else ax.set_ylabel('RMSE')\n",
        "  ax.plot(range(1, T+1), accuracy_baseline, marker='o', label='Baseline')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_0, marker='o', label='VCL - No Coresets')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_200, marker='o', label='VCL - Coreset Size 200')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_400, marker='o', label='VCL - Coreset Size 400')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_1000, marker='o', label='VCL - Coreset Size 1000')\n",
        "  ax.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  plt.savefig(f\"{experiment_name}_accuracies.png\")\n",
        "\n",
        "def plot_metrics(metrics, experiment_name):\n",
        "  metrics_baseline = metrics[0]\n",
        "  metrics_vcl_0 = metrics[1]\n",
        "  metrics_vcl_200 = metrics[2]\n",
        "  metrics_vcl_400 = metrics[3]\n",
        "  metrics_vcl_1000 = metrics[4]\n",
        "\n",
        "  x = np.arange(3)\n",
        "  width = 0.2\n",
        "  fig, ax = plt.subplots()\n",
        "  rects1 = ax.bar(x - width*2.5, metrics_baseline, width, label='Baseline')\n",
        "  rects2 = ax.bar(x - width/2, metrics_vcl_0, width, label='VCL - No Coresets')\n",
        "  rects3 = ax.bar(x + width/2, metrics_vcl_200, width, label='VCL - Coreset Size 200')\n",
        "  rects4 = ax.bar(x + width*2.5, metrics_vcl_400, width, label='VCL - Coreset Size 400')\n",
        "  rects5 = ax.bar(x + width*5, metrics_vcl_1000, width, label='VCL - Coreset Size 1000')\n",
        "  ax.set_ylabel('Metric')\n",
        "  ax.set_title(experiment_name)\n",
        "  ax.set_xticks(x)\n",
        "  ax.set_xticklabels(['Avg Accuracy', 'BWT', 'FM'])\n",
        "  ax.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  plt.savefig(f\"{experiment_name}_metrics.png\")"
      ],
      "metadata": {
        "id": "JQRt_OxbhA9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_combined_accuracies(accuracies, accuracies_regression, accuracies_ws2, permuted=True):\n",
        "  accuracy_baseline = accuracies[0]\n",
        "  accuracy_vcl_0 = accuracies[1]\n",
        "  accuracy_vcl_200 = accuracies[2]\n",
        "  accuracy_vcl_400 = accuracies[3]\n",
        "  accuracy_vcl_1000 = accuracies[4]\n",
        "\n",
        "  accuracy_baseline_reg = accuracies_regression[0]\n",
        "  accuracy_vcl_0_reg = accuracies_regression[1]\n",
        "  accuracy_vcl_200_reg = accuracies_regression[2]\n",
        "  accuracy_vcl_400_reg = accuracies_regression[3]\n",
        "  accuracy_vcl_1000_reg = accuracies_regression[4]\n",
        "\n",
        "  accuracy_vcl_0_ws2 = accuracies_ws2[0]\n",
        "  accuracy_vcl_200_ws2 = accuracies_ws2[1]\n",
        "  accuracy_vcl_400_ws2 = accuracies_ws2[2]\n",
        "  accuracy_vcl_1000_ws2 = accuracies_ws2[3]\n",
        "\n",
        "  T = len(accuracy_baseline)\n",
        "  exp = 'Permuted' if permuted else 'Split'\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(8, 5))\n",
        "  ax.set_title(f'{exp} MNIST')\n",
        "  ax.set_xlabel('Task')\n",
        "  ax.set_ylabel('Accuracy')\n",
        "  ax.plot(range(1, T+1), accuracy_baseline, marker='o', label='Baseline')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_0, marker='o', label=f'VCL - No Coresets')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_200, marker='o', label=f'VCL - Coreset Size 200')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_400, marker='o', label=f'VCL - Coreset Size 400')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_1000, marker='o', label=f'VCL - Coreset Size 1000')\n",
        "\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_0_reg, marker='x', label=f'Regression VCL')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_200_reg, marker='x', label=f'Regression VCL - 200')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_400_reg, marker='x', label=f'Regression VCL - 400')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_1000_reg, marker='x', label=f'Regression VCL - 1000')\n",
        "\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_0_ws2, marker='^', label=f'WS VCL')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_200_ws2, marker='^', label=f'WS VCL - 200')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_400_ws2, marker='^', label=f'WS VCL - 400')\n",
        "  ax.plot(range(1, T+1), accuracy_vcl_1000_ws2, marker='^', label=f'WS VCL - 1000')\n",
        "\n",
        "  ax.legend(loc='lower left')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  fig.savefig(f\"{exp}_accuracies.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GAC8mtqOMJ52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Permuted"
      ],
      "metadata": {
        "id": "OdL-npjerjZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp = 'accuracy_matrix_permuted'\n",
        "matrices_perm = [accuracy_matrices[exp + '_baseline.npy'], accuracy_matrices[exp + '_vcl_coreset_0.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_200.npy'], accuracy_matrices[exp + '_vcl_coreset_400.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_1000.npy']]\n",
        "\n",
        "metrics_perm = [cl_metrics(M) for M in matrices_perm]\n",
        "accuracies_perm = [m[0] for m in metrics_perm]\n",
        "AA_perm = metrics_perm[-1][1]\n",
        "BWT_perm = metrics_perm[-1][2]\n",
        "FM_perm = metrics_perm[-1][3]\n",
        "\n",
        "exp = 'accuracy_matrix_permuted_regression'\n",
        "matrices_perm_reg = [accuracy_matrices[exp + '_baseline.npy'], accuracy_matrices[exp + '_vcl_coreset_0.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_200.npy'], accuracy_matrices[exp + '_vcl_coreset_400.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_1000.npy']]\n",
        "\n",
        "\n",
        "\n",
        "metrics_perm_regression = [cl_metrics(M) for M in matrices_perm]\n",
        "accuracies_perm_regression = [m[0] for m in metrics_perm_regression]\n",
        "AA_perm_regression = metrics_perm_regression[-1][1]\n",
        "BWT_perm_regression = metrics_perm_regression[-1][2]\n",
        "FM_perm_regression = metrics_perm_regression[-1][3]\n",
        "\n",
        "exp = 'accuracy_matrix_permuted_reg_ws_2'\n",
        "matrices_perm_ws_2 = [accuracy_matrices[exp + '_vcl_coreset_0.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_200.npy'], accuracy_matrices[exp + '_vcl_coreset_400.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_1000.npy']]\n",
        "\n",
        "metrics_perm_ws_2 = [cl_metrics(M) for M in matrices_perm_ws_2]\n",
        "accuracies_perm_ws_2 = [m[0] for m in metrics_perm_ws_2]\n",
        "AA_perm_ws_2 = metrics_perm_ws_2[-1][1]\n",
        "BWT_perm_ws_2 = metrics_perm_ws_2[-1][2]\n",
        "FM_perm_ws_2 =  metrics_perm_ws_2[-1][3]\n",
        "\n",
        "\n",
        "AA_list = [AA_perm, AA_perm_regression, AA_perm_ws_2]\n",
        "BWT_list = [BWT_perm, BWT_perm_regression, BWT_perm_ws_2]\n",
        "FM_list = [FM_perm, FM_perm_regression, FM_perm_ws_2]\n",
        "\n",
        "plot_combined_accuracies(accuracies_perm, accuracies_perm_regression, accuracies_perm_ws_2, permuted=True)\n",
        "\n",
        "print(f'Accuracy VCL = {AA_perm:.3f}')\n",
        "print(f'Accuracy VCL Regression = {AA_perm_regression:.3f}')\n",
        "print(f'Accuracy VCL Wasserstein = {AA_perm_ws_2:.3f}')\n"
      ],
      "metadata": {
        "id": "j7vYgiKmOX2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split"
      ],
      "metadata": {
        "id": "IJAq-QMQrkmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp = 'accuracy_matrix_split'\n",
        "matrices_perm = [accuracy_matrices[exp + '_baseline.npy'], accuracy_matrices[exp + '_vcl_coreset_0.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_200.npy'], accuracy_matrices[exp + '_vcl_coreset_400.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_1000.npy']]\n",
        "\n",
        "metrics_perm = [cl_metrics(M) for M in matrices_perm]\n",
        "accuracies_perm = [m[0] for m in metrics_perm]\n",
        "AA_perm = metrics_perm[-1][1]\n",
        "BWT_perm = metrics_perm[-1][2]\n",
        "FM_perm = metrics_perm[-1][3]\n",
        "\n",
        "exp = 'accuracy_matrix_split_regression'\n",
        "matrices_perm_reg = [accuracy_matrices[exp + '_baseline.npy'], accuracy_matrices[exp + '_vcl_coreset_0.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_200.npy'], accuracy_matrices[exp + '_vcl_coreset_400.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_1000.npy']]\n",
        "\n",
        "\n",
        "\n",
        "metrics_perm_regression = [cl_metrics(M) for M in matrices_perm]\n",
        "accuracies_perm_regression = [m[0] for m in metrics_perm_regression]\n",
        "AA_perm_regression = metrics_perm_regression[-1][1]\n",
        "BWT_perm_regression = metrics_perm_regression[-1][2]\n",
        "FM_perm_regression = metrics_perm_regression[-1][3]\n",
        "\n",
        "exp = 'accuracy_matrix_split_reg_ws_2'\n",
        "matrices_perm_ws_2 = [accuracy_matrices[exp + '_vcl_coreset_0.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_200.npy'], accuracy_matrices[exp + '_vcl_coreset_400.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_1000.npy']]\n",
        "\n",
        "metrics_perm_ws_2 = [cl_metrics(M) for M in matrices_perm_ws_2]\n",
        "accuracies_perm_ws_2 = [m[0] for m in metrics_perm_ws_2]\n",
        "AA_perm_ws_2 = metrics_perm_ws_2[-1][1]\n",
        "BWT_perm_ws_2 = metrics_perm_ws_2[-1][2]\n",
        "FM_perm_ws_2 =  metrics_perm_ws_2[-1][3]\n",
        "\n",
        "\n",
        "AA_list = [AA_perm, AA_perm_regression, AA_perm_ws_2]\n",
        "BWT_list = [BWT_perm, BWT_perm_regression, BWT_perm_ws_2]\n",
        "FM_list = [FM_perm, FM_perm_regression, FM_perm_ws_2]\n",
        "\n",
        "plot_combined_accuracies(accuracies_perm, accuracies_perm_regression, accuracies_perm_ws_2, permuted=False)\n",
        "\n",
        "print(f'Accuracy VCL = {AA_perm:.3f}')\n",
        "print(f'Accuracy VCL Regression = {AA_perm_regression:.3f}')\n",
        "print(f'Accuracy VCL Wasserstein = {AA_perm_ws_2:.3f}')\n"
      ],
      "metadata": {
        "id": "Pb0oXLqmhyko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSE"
      ],
      "metadata": {
        "id": "qarVh69k5w9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp = 'accuracy_matrix_permuted_regression'\n",
        "matrices_perm = [accuracy_matrices[exp + '_baseline.npy'], accuracy_matrices[exp + '_vcl_coreset_0.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_200.npy'], accuracy_matrices[exp + '_vcl_coreset_400.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_1000.npy']]\n",
        "matrices_perm_rmse = [m[10:, :] for m in matrices_perm]\n",
        "\n",
        "metrics_perm = [cl_metrics(M) for M in matrices_perm_rmse]\n",
        "accuracies_perm = [m[0] for m in metrics_perm]\n",
        "\n",
        "plot_accuracies(accuracies_perm, 'Permuted MNIST', is_regression=True)"
      ],
      "metadata": {
        "id": "U75Vpjg95zQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = 'accuracy_matrix_split_regression'\n",
        "matrices_perm = [accuracy_matrices[exp + '_baseline.npy'], accuracy_matrices[exp + '_vcl_coreset_0.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_200.npy'], accuracy_matrices[exp + '_vcl_coreset_400.npy'], \\\n",
        "                                 accuracy_matrices[exp + '_vcl_coreset_1000.npy']]\n",
        "matrices_perm_rmse = [m[5:, :] for m in matrices_perm]\n",
        "\n",
        "metrics_perm = [cl_metrics(M) for M in matrices_perm_rmse]\n",
        "accuracies_perm = [m[0] for m in metrics_perm]\n",
        "\n",
        "plot_accuracies(accuracies_perm, 'Split MNIST', is_regression=True)"
      ],
      "metadata": {
        "id": "WydILBQj7WZP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "rvxzBXTuBWSt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
