# Overview

This repository is a PyTorch implementation of Variational Continual Learning [1]](#1). As an additional modification in the latter experiments, the KL-divergence objective between successive weights from the original paper has been replaced by one based on moment matching. This is theoretically motivated from Wasserstein-distance minimisation. The proof is as follows:

In my setup, the marginals of the joint distribution Î³ over each set of weights (Î¸ ~ q, Î¸â€² ~ q_{tâˆ’1}) is Gaussian:

âˆ«_Î¸ Î³(Î¸, Î¸â€²) dÎ¸     = q_{tâˆ’1}(Î¸â€²) = ğ’©(Î¸â€²; Î¼_{tâˆ’1}, Ïƒ_{tâˆ’1}Â²)  
âˆ«_{Î¸â€²} Î³(Î¸, Î¸â€²) dÎ¸â€² = q(Î¸)       = ğ’©(Î¸; Î¼, ÏƒÂ²)

We compute the following expectation under Î³:

E_{Î¸, Î¸â€² ~ Î³} [ Î£_{n=1}^{N_t} âˆ’log p(y_t^{(n)} | Î¸, x_t^{(n)}) + Î»(Î¸ âˆ’ Î¸â€²)Â² ]

Expanding this:

= âˆ¬ Î£_{n=1}^{N_t} âˆ’log p(y_t^{(n)} | Î¸, x_t^{(n)}) Î³(Î¸, Î¸â€²) dÎ¸ dÎ¸â€²  
  + Î» E_{Î¸, Î¸â€² ~ Î³} [Î¸Â² + Î¸â€²Â² âˆ’ 2Î¸Î¸â€²]

= âˆ« Î£_{n=1}^{N_t} âˆ’log p(y_t^{(n)} | Î¸, x_t^{(n)}) q(Î¸) dÎ¸  
  + Î» ( E_Î³[Î¸Â²] + E_Î³[Î¸â€²Â²] âˆ’ 2E_Î³[Î¸Î¸â€²] )

= E_q[ Î£_{n=1}^{N_t} âˆ’log p(y_t^{(n)} | Î¸, x_t^{(n)}) ]  
  + Î» [ Î¼Â² + ÏƒÂ² + Î¼_{tâˆ’1}Â² + Ïƒ_{tâˆ’1}Â² âˆ’ 2(Ï_{Î¸Î¸â€²} Ïƒ Ïƒ_{tâˆ’1} + Î¼ Î¼_{tâˆ’1}) ]

The last term uses the identity:

Ï_{XY} = Cov(X, Y) / (Ïƒ_X Ïƒ_Y) = E[(X âˆ’ E[X])(Y âˆ’ E[Y])] / (Ïƒ_X Ïƒ_Y)

Finally, since both distributions are from the same family, for the optimisation objective of aligning them, we take the case when the correlation is 1, giving the final objective after completing the square:

E_q[ Î£_{n=1}^{N_t} âˆ’log p(y_t^{(n)} | Î¸, x_t^{(n)}) ]  
+ Î» [ (Î¼ âˆ’ Î¼_{tâˆ’1})Â² + (Ïƒ âˆ’ Ïƒ_{tâˆ’1})Â² ]



## References
<a id="1">[1]</a> Nguyen, Cuong V., et al. "Variational continual learning." arXiv preprint arXiv:1710.10628 (2017).
